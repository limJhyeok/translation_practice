{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my github: https://github.com/withAnewWorld/models_from_scratch\n",
    "# my blog\n",
    "# https://self-deeplearning.blogspot.com/\n",
    "# https://self-deeplearning.tistory.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqOHdkfQrcGI",
    "outputId": "27a952a1-732b-4dcc-dda4-a7fdf2bced81",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/translation\n"
     ]
    }
   ],
   "source": [
    "# colab과 google drive를 연동하기 위한 code cell입니다.\n",
    "# google drive를 연동하는 이유는 drive에 저장된 dataset을 가져오기 위함입니다.\n",
    "\n",
    "from google.colab import drive\n",
    "import sys\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "FOLDERNAME = 'translation'\n",
    "sys.path.append('content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "%cd /content/drive/My Drive/$FOLDERNAME\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwvM-bKDzhvB"
   },
   "source": [
    "## ref\n",
    "1. embedding <br>\n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html<br>\n",
    "paper: https://arxiv.org/abs/1301.3781\n",
    "2. Seq2Seq <br>\n",
    "0) paper: https://arxiv.org/abs/1409.3215<br>\n",
    "1) Pytorch Seq2Seq Tutorial for Machine Translation(Youtuber: \n",
    "Aladdin Persson): <br>\n",
    "https://www.youtube.com/watch?v=EoGUlvhRYpk&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=38<br>\n",
    "2) github: <br>\n",
    "https://github.com/bentrevett/pytorch-seq2seq <br>\n",
    "3) PyTorch tutorial<br>\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html<br>\n",
    "4) seq2seq attention <br>\n",
    "https://arxiv.org/abs/1409.0473?context=cs.NE <br>\n",
    "5) avoid overfitting method(discuss torch): <br>\n",
    "https://discuss.pytorch.org/t/simple-encoder-decoder-model-is-overfitting/74632<br>\n",
    "3. RNN, LSTM <br>\n",
    "1)https://colah.github.io/posts/2015-08-Understanding-LSTMs/ <br>\n",
    "2)https://cs231n.github.io/rnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "1. Data Download <br>\n",
    "2. Seq2Seq architecture\n",
    "    - tokenize\n",
    "    - embedding\n",
    "3. Data preprocessing <br>\n",
    "4. Seq2Seq architecture\n",
    "    - Context Vector\n",
    "    - Encoder RNN\n",
    "    - Decoder RNN\n",
    "5. How to train & evaluate model in PyTroch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 들어가기 전에\n",
    "data preprocessing은 PyTorch 공식 Tutorial 코드를 대부분 복사 & 붙여넣기 했습니다. <br>\n",
    "따라서 data preprocessing 설명이 부족할 경우  <br>\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html  <br>\n",
    "를 참고해주시면 감사하겠습니다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKIHiiU0mIjI"
   },
   "source": [
    "## Loading data files\n",
    "The data for this project is a set of many thousands of English to French translation pairs.\n",
    "\n",
    "This question on Open Data Stack Exchange <https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>__ pointed me to the open translation site https://tatoeba.org/ which has downloads available at https://tatoeba.org/eng/downloads - and better yet, someone did the extra work of splitting language pairs into individual text files here: https://www.manythings.org/anki/\n",
    "\n",
    "The English to French pairs are too big to include in the repo, so download to data/eng-fra.txt before continuing. The file is a tab separated list of translation pairs:\n",
    "\n",
    "::\n",
    "\n",
    "I am cold.    J'ai froid.\n",
    ".. Note:: **Download the data from here** <https://download.pytorch.org/tutorial/data.zip>_ and extract it to the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zug2Y_i9k7a"
   },
   "source": [
    "## Seq2Seq\n",
    "번역(translation)문제를 해결하는 데에 기본적 구조인 Seq2Seq 모델은 대략적으로 다음과 같은 구조를 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/seq2seq.PNG\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eFEJ5JZ9viM"
   },
   "source": [
    "먼저 Tokenize, Embeding에 대해 대략적으로 알아보고 Encoder RNN, Decoder RNN 등에 대해 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75_PNeZXWfzX"
   },
   "source": [
    "## Tokenize\n",
    "문장과 같은 순서를 가지는 데이터(sequential data)를 일정 기준을 통해 자르는 행위. <br>\n",
    "\n",
    "일반적으로 번역 문제에서는 띄어쓰기 또는 의미단위(형태소)를 기준으로 자릅니다.<br>\n",
    "\n",
    "Tokenize의 필요성에 대한 직관적인 이해로는 사람 또한 문장을 읽을 때 자연스럽게 띄어쓰기 또는 의미 단위로 문장을 자른다는 것입니다. <br>\n",
    "\n",
    "**I / have to submit / the assignment / until tomorrow** <br>\n",
    "\n",
    "위의 문장을 해석할 때, 문장의 성분(주어, 목적어, 보어 등)에 따라 해석을 하신 경험이 있으실 것입니다. <br>\n",
    "이와 같이 인공신경망인 RNN 또한 Tokenize된 문장을 통해 성능을 높일 수 있습니다. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hObu2RibhUu"
   },
   "source": [
    "## Embedding\n",
    "문장과 같은 수치로 표현되지 않은 데이터를 수치(벡터)로 표현하는 방법<br>\n",
    "### 1. one hot encoding\n",
    "기존에 많이 쓰이던 방법으로는 one hot encoding이 있습니다 <br>\n",
    "\n",
    "```python\n",
    "import copy\n",
    "one_hot_encoding = [0] * (number_of_tokens)\n",
    "embed = {}\n",
    "for i in range(number_of_tokens):\n",
    "  embed[tokens[i]] = copy.deepcopy(one_hot_encoding)\n",
    "  embed[tokens[i]][i] = 1 \n",
    "\n",
    "# ex) number_of_tokens = 3\n",
    "# token_0 = (1, 0, 0), token_1 = (0, 1, 0), token_2 = (0, 0, 1)\n",
    "```\n",
    "one hot encoding의 문제점\n",
    "> 1) 컴퓨터 자원(메모리) 낭비 <br>\n",
    "  대부분의 embeding vector에 의미를 가지지 않는 0이 할당 <br>\n",
    "\n",
    "> 2) 단어간의 유사도 계산 불가<br>\n",
    "  벡터간의 유사도는 쉽게 내적을 통해 구할 수 있습니다. <br>\n",
    "  즉, one hot encoding은 모든 embedding이 서로 유사도를 가지지 않습니다. $(0, 1)\\cdot(1, 0) = 0$ <br>\n",
    "\n",
    "  이의 문제점은 <br>\n",
    "\n",
    ">  1) '**나는**', '**나를**' 과 같이 대부분의 문장에서 비슷한 의미를 가지는 단어에 대해 학습하기 힘들다. <br>\n",
    "\n",
    ">  2) 문맥에 따라 의미가 유사한 단어들에 대해 학습하기 어렵다. <br>\n",
    "  ex) '늦게까지 공부를 하느라 **눈이 감긴다.**'<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; '늦게까지 공부를 하느라 **피곤하다.**'\n",
    "\n",
    "이에 따라 단어간의 유사도를 반영할 수 있으면서 컴퓨터 자원을 효율적으로 사용할 수 있는 Embedding 방법론이 필요하게 되었습니다. <br>\n",
    "### 2. Lookup table\n",
    "-> idea: ramdom vector(N X embedding dim)를 만들어서 단어를 인덱스로 사용하면 되지 않을까?\n",
    "\n",
    "key point: 인공신경망은 학습을 통해 개선된다! 프로그래머의 룰이 아니라 <br>\n",
    "\n",
    "random initialize -> 학습 -> better result!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/embedding.PNG\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KH2xZ6pJ_B6x"
   },
   "source": [
    "embedding vector는 학습이 되면서 점차 단어간 유사도가 높은 경우 매우 높은 유사도(1)를 도출하게 vector의 값들이 변하게 됩니다. <br>\n",
    "(반대의 경우 -1) <br>\n",
    "흥미로운 점은 적절하게 학습된 embedding vector간의 연산은 사람이 직관적으로 이해할 수 있다는 점입니다. <br>\n",
    "ex) vector(king) - vector(man) + vector(woman) $\\simeq$ vector(queen) <br>\n",
    "ref) Efficient Estimation of Word Representations in\n",
    "Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O_UjhUqm3i-R"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6Gj9vsQd3dkd"
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "  def __init__(self, name):\n",
    "    '''\n",
    "    name(str): Language's name (ex. eng, fra)\n",
    "    '''\n",
    "    self.name = name\n",
    "    self.word2index = {\"<SOS>\": 0, \"<EOS>\": 1, \"<PAD>\": 2}\n",
    "    self.word2count = {\"<SOS>\": 0, \"<EOS>\": 0, \"<PAD>\": 0}\n",
    "    self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2: \"<PAD>\"}\n",
    "    self.n_words = 3\n",
    "\n",
    "  def addSentence(self, sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.addWord(word)\n",
    "\n",
    "  def addWord(self, word):\n",
    "    if word not in self.word2index:\n",
    "      self.word2index[word] = self.n_words\n",
    "      self.word2count[word] = 1\n",
    "      self.index2word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "    else:\n",
    "      self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GnwUT5t34Izn"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JlQ6bvDu4J_K"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse = False):\n",
    "  '''\n",
    "  open txt file and return each of Lang class, pair lists\n",
    "  inputs\n",
    "    - lang1(str): language's name\n",
    "    - lang2(str): language's name\n",
    "    - reverse(bool): lang2 -> lang1 if reverse else lang1 -> lang2 (->: translation) \n",
    "  returns\n",
    "    - input_lang(class)\n",
    "    - output_lang(class)\n",
    "    - pairs(list): [lang1 sentence, lang2 sentence] * (n_sentences) in txt file\n",
    "  '''\n",
    "\n",
    "  print('Reading lines...')\n",
    "  with open(os.path.join(os.getcwd(), 'data/%s-%s.txt'%(lang1, lang2)), encoding = 'utf-8') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "\n",
    "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "  if reverse:\n",
    "    pairs = [list(reversed(p)) for p in pairs]\n",
    "    input_lang = Lang(lang2)\n",
    "    output_lang = Lang(lang1)\n",
    "  else:\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "  return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vbLMnPKidRO"
   },
   "source": [
    "## filter data\n",
    "빠른 학습과 model의 성능을 끌어올리기 위해 문장 token의 최대 갯수를 제한하고 단순한 구조의 문장('I am ~, He is ~')을 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HMd9T2NP43pe"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p, max_length):\n",
    "  '''\n",
    "  filter pair(in/output sentence)\n",
    "  1. n_tokens of pair < MAX_LENGTH - 2 for <SOS>, <EOS> token\n",
    "  2. output sentence(eng) starts with first pharse in eng_prefixes \n",
    "  input:\n",
    "    - p(list): [intput sentence, output sentence]\n",
    "  output:\n",
    "    - filtered pair(list): [input sentence, output sentence]\n",
    "  '''\n",
    "  return len(p[0].split(' ')) < (MAX_LENGTH - 2) and \\\n",
    "      len(p[1].split(' ')) < (MAX_LENGTH  - 2)and \\\n",
    "      p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs, max_length):\n",
    "  '''\n",
    "  filter all pairs \n",
    "  '''\n",
    "  return [pair for pair in pairs if filterPair(pair, max_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8rJma4di2LK"
   },
   "source": [
    "## data preprocessing\n",
    "> 0) make Lang class of each lang1 and lang2 <br>\n",
    "\n",
    "> 1) data filter <br>\n",
    "\n",
    "> 2) add special token ( SOS(Start Of Sequence(or Sentence)), EOS(End Of Sequence), PAD )  <br>\n",
    "  SOS token: 문장의 처음을 알리는 Token.<br>\n",
    "  EOS token: 문장의 끝을 알리는 Token<br>\n",
    "  PAD token: 여러 문장을 묶기 위한 token. 각 문장의 token size를 맞춰주는 역할. <br>\n",
    "\n",
    "각 문장의 Token(word)의 갯수가 모두 다르므로 이를 묶기 위해서는 동일한 크기로 맞춰줘야 합니다. (병렬 처리를 위해)<br>\n",
    "이를 위해 PAD Token을 사용합니다. <br>\n",
    "\n",
    "cf) random_shuffle <br>\n",
    "데이터셋인 txt파일을 보시면 비슷한 문장이 계속 나열되는 것을 아실 수 있습니다. <br>\n",
    "데이터셋을 train, validation으로 나눌 때 데이터 분포가 극명하게 갈리게 되면 overfitting을 유발하므로 문장의 순서를 섞어 방지합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1cN_GkX48r3",
    "outputId": "b9734538-8ec9-4a4b-e5b9-d2acedc914d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 3383\n",
      "eng 2134\n",
      "['<SOS> tu es fort effronte . <EOS> <PAD> <PAD> <PAD>', '<SOS> you re very forward . <EOS> <PAD> <PAD> <PAD>']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, max_length, reverse = False, random_shuffle = False):\n",
    "  '''\n",
    "  inputs:\n",
    "    - lang1(str): input language type (ex. eng)\n",
    "    - lang2(str): output language type (ex. fra)\n",
    "    - max_length(int): max length of tokens in each sentence\n",
    "    - reverse(bool): lang2 -> lang1 if reverse else lang1 -> lang2 (->: translation) \n",
    "    - random_shuffle(bool): sentence(dataset) will be shuffled randomly\n",
    "  outputs:\n",
    "    - input_lang(class): class Lang()\n",
    "    - output_lang(class): class Lang()\n",
    "    - pairs(list): [input sentence, output sentence]\n",
    "  '''\n",
    "\n",
    "  input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "  print('Read %s sentence pairs' % len(pairs))\n",
    "  pairs = filterPairs(pairs, max_length)\n",
    "  # print('Trimmed to %s sentence pairs' %(pairs))\n",
    "  print('Counting words...')\n",
    "\n",
    "  # for parallel computation use pad(fix every sentences' length as max_length)\n",
    "  for pair in pairs:\n",
    "    pair[0] = '<SOS> ' + pair[0] \n",
    "    pair[1] = '<SOS> ' + pair[1] \n",
    "    pair[0] += ' <EOS>'\n",
    "    pair[1] += ' <EOS>'\n",
    "    if len(pair[0].split(' ')) < max_length:\n",
    "      n_pad = max_length - len(pair[0].split(' ')) \n",
    "      pair[0] += ' <PAD>' * n_pad\n",
    "    if len(pair[1].split(' ')) < max_length:\n",
    "      n_pad = max_length - len(pair[1].split(' '))\n",
    "      pair[1] += ' <PAD>' * n_pad  \n",
    "    input_lang.addSentence(pair[0])\n",
    "    output_lang.addSentence(pair[1])\n",
    "\n",
    "  print('Counted words:')\n",
    "  print(input_lang.name, input_lang.n_words)\n",
    "  print(output_lang.name, output_lang.n_words)\n",
    "  if random_shuffle:\n",
    "    random.shuffle(pairs)\n",
    "  return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', MAX_LENGTH, True, True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOZNYxAekBjo"
   },
   "source": [
    "## index to Tensor\n",
    "문장의 각 token index를 torch.tensor로 변환합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pKRSqH3f55mO"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "  return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "  indexes = indexesFromSentence(lang, sentence)\n",
    "  # indexes.append(EOS_token)\n",
    "  return torch.tensor(indexes, dtype = torch.long, device = device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "  input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "  target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "  return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2GSanye6jLZ"
   },
   "source": [
    "번역(translation) 문제의 경우 입력 문장(input sentence)과 출력 문장(output sentence)을 mapping 시키는 문제로 생각할 수 있습니다. <br>\n",
    "ex) (Life is short.) -> (인생은 짧다.) <br>\n",
    "\n",
    "위에서 설명드린대로 RNN은 문장과 같은 Sequneital data를 처리하는 인공신경망으로 input sentence를 하나의 sequential data, <br> output sentence를 또 하나의 sequential data로 생각하면 다음과 같은 idea가 떠오르실 것입니다. <br>\n",
    "RNN(input sentence) -> RNN -> output sentence <br>\n",
    "즉, Seq2Seq는 하나의 인공신경망 RNN을 통해 input sentence를 처리한 후, <br>\n",
    "그 결과값(context vector)을 output sentence를 처리하는 RNN에 전달하는 방식으로 번역문제를 다룰 수 있습니다. <br>\n",
    "\n",
    "cf) context vector<br>\n",
    "input sentence의 token을 Encoder RNN에 feed한 후에 도출되는 hidden state인 context vector는 문장의 모든 정보(맥락)을 가질 것으로 생각할 수 있습니다. <br>\n",
    "다만 문장의 제일 처음에 나오는 token의 경우 연산이 누적되기 때문에 정보의 손실이 많아집니다. <br>\n",
    "즉 기본적인 Seq2Seq 모델의 경우 문장의 길이가 긴 경우 번역이 적절하게 되지 않는 특성을 가지고 있습니다. <br>\n",
    "ref: https://arxiv.org/abs/1409.0473?context=cs.NE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/seq2seq_.PNG\">\n",
    "</p>\n",
    "<div align = 'center'> 자료출처: https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm6gcDJBL_Qy"
   },
   "source": [
    "## RNN\n",
    "$h_{t} = sigmoid(W_{hx}x_{t} + W_{hh}h_{t-1})$ <br>\n",
    "$y_{t} = W_{yh}h_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/RNN.PNG\" height = 300>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdUbbBiYct45"
   },
   "source": [
    "RNN의 경우 gradient vanishing / exploding 문제가 빈번하게 발생하기 때문에 이를 부분적으로 해결하기 위해 LSTM이 개발되었습니다. <br>\n",
    "LSTM의 자세한 작동 원리에 대해서는 설명하지 않고 RNN과 비슷하게 작동하며, <br> hidden state, input data와 함께 cell state 속성을 추가로 필요하다는 것만 설명드리고 계속 진행하겠습니다. <br>\n",
    "\n",
    "$h_{t} = RNN(e(x_{t}), h_{t-1})$ <br>\n",
    "\n",
    "$(h_{t}, c_{t}) = LSTM(e(x_{t}), h_{t-1}, c_{t-1})$ <br>\n",
    "(e: embedding, h: hidden state, c: cell state, x: input ) <br>\n",
    "<br>\n",
    "\n",
    "for more detail of LSTM and RNN<br>\n",
    " 1. https://colah.github.io/posts/2015-08-Understanding-LSTMs/ <br>\n",
    "\n",
    " 2. https://cs231n.github.io/rnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2-u_M29kQoc"
   },
   "source": [
    "## Encoder RNN & Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "6fzZw-Nr_P0T"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               seq_length,\n",
    "               emb_dim,\n",
    "               hid_dim,\n",
    "               n_tokens,\n",
    "               num_layers = 1,\n",
    "               drop_p = 0.3):\n",
    "    '''\n",
    "    inputs:\n",
    "      - seq_length(int): seq_length of src \n",
    "      - emb_dim(int): emb_dim of embedding matrix\n",
    "      - hid_dim(int): hidden dim of rnn\n",
    "      - n_tokens(int): total number of tokens in soruce\n",
    "      - num_layers(int): num_layers of rnn\n",
    "      - drop_p(float): drop_p after embeddding\n",
    "    \n",
    "    '''\n",
    "\n",
    "    super(Encoder, self).__init__()\n",
    "    self.embedding = nn.Embedding(n_tokens, emb_dim)\n",
    "    self.dropout = nn.Dropout(drop_p)\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers)\n",
    "\n",
    "  def forward(self, src):\n",
    "    '''\n",
    "    inputs:\n",
    "      - src(Tensor(seq_length, batch_size))\n",
    "    outputs:\n",
    "      - outputs(Tensor[seq_length, batch_size, emb_dim])\n",
    "      - hidden(Tensor[1, batch_size, hid_dim])\n",
    "      - cell(Tensor[1, batch_size, hid_dim])\n",
    "    '''\n",
    "    embedded = self.dropout(self.embedding(src)) # (seq_length, batch_size) -> (seq_length, batch_size, emb_dim)\n",
    "    outputs, (hidden, cell) = self.rnn(embedded)\n",
    "    return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "_kWijXBE_TSQ"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               seq_length,\n",
    "               emb_dim,\n",
    "               hid_dim,\n",
    "               trg_n_tokens,\n",
    "               drop_p = 0):\n",
    "    '''\n",
    "    inputs:\n",
    "      - seq_length(int): seq_length of target\n",
    "      - emb_dim(int): emb_dim of embedding matrix\n",
    "      - hid_dim(int): hidden dim of rnn\n",
    "      - trg_n_tokens(int): total number of tokens in target\n",
    "      - drop_p(float): drop_p after embeddding\n",
    "    '''\n",
    "\n",
    "    super(Decoder, self).__init__()\n",
    "    self.seq_length = seq_length\n",
    "    self.trg_n_tokens = trg_n_tokens\n",
    "    self.embedding = nn.Embedding(trg_n_tokens, emb_dim)\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim)\n",
    "    self.fc = nn.Linear(hid_dim, trg_n_tokens)\n",
    "    self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "  def forward(self, trg, hidden, cell):\n",
    "    '''\n",
    "    inputs:\n",
    "      - trg(Tensor[batch_size])\n",
    "      - hidden(Tensor[1, batch_size, hid_dim])\n",
    "      - cell(Tensor[1, batch_size, hid_dim])\n",
    "    returns:\n",
    "      - output(Tensor[batch_size, n_tokens]):\n",
    "      - hidden(Tensor[1, batch_size, hid_dim]):\n",
    "      - cell(Tensor[1, batch_size, hid_dim])\n",
    "    '''\n",
    "    trg = trg.unsqueeze(0) # (1, batch_size)\n",
    "    embedded = self.dropout(self.embedding(trg)) # (1, batch_size, emb_dim)\n",
    "    output, (hidden, cell) = self.rnn(embedded, (hidden, cell)) # output: (1, batch_size, hid_dim) \n",
    "    output = output.squeeze(0) \n",
    "    output = self.fc(output) # output: (batch_size, n_tokens)\n",
    "    return output, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ7Dc52Y4icf"
   },
   "source": [
    "## Decoder RNN in Seq2Seq\n",
    "Decoder의 경우 context vector와 SOS Token을 시작으로 output을 예측해야 합니다. <br>\n",
    "각 결과값과 hidden state를 Decoder에 feed함으로써 output sentence를 생성해야 하므로 Encoder RNN과 다르게 반복문을 통해서 제어를 해야합니다. <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/decoder.PNG\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EckMty7g2nQ"
   },
   "source": [
    "다만 output sentence의 정답 token(ground truth token)을 랜덤으로 feed하여 model을 올바르게, 빠르게 학습할 수 있습니다. <br>\n",
    "이 확률을 teacher forcing ratio로 설정합니다. <br>\n",
    "**주의**<br>\n",
    "모델의 성능을 평가(evaluation)할 때 teacher forcing은 사용하면 안됩니다!! <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "irlkdnQH_YS2"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self,\n",
    "               encoder,\n",
    "               decoder,\n",
    "               device):\n",
    "    super(Seq2Seq, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.device = device\n",
    "\n",
    "  def forward(self, src, trg, teacher_forcing_ratio = 0.75):\n",
    "    '''\n",
    "    inputs:\n",
    "      - src(Tensor[src_seq_length, batch_size])\n",
    "      - trg(Tensor[trg_seq_length, batch_size])\n",
    "      - teacher_forcing_ratio(float): input of decoder will be ground truths token or prediction following by ratio\n",
    "    '''\n",
    "\n",
    "    outputs, hidden, cell = self.encoder(src)\n",
    "    seq_len = trg.size(0)\n",
    "    batch_size = trg.size(1)\n",
    "    trg_n_tokens = self.decoder.trg_n_tokens\n",
    "\n",
    "    logits = torch.zeros((seq_len, batch_size, trg_n_tokens)).to(device)\n",
    "    input = trg[0, :] # input(Tensor[batch_size])\n",
    "    for i in range(1, seq_len):\n",
    "      output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "      logits[i] = output\n",
    "      top1 = output.argmax(1)\n",
    "      if self.training: # We should not use teacher forcing when eval\n",
    "        input = trg[i] if random.random() < teacher_forcing_ratio else top1\n",
    "      else:\n",
    "        input = top1\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BeZpZUGBmxDl"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "n_pairs = len(pairs)\n",
    "\n",
    "src = torch.zeros((MAX_LENGTH, n_pairs), dtype = torch.long) \n",
    "trg = torch.zeros((MAX_LENGTH, n_pairs), dtype = torch.long)\n",
    "for i in range(n_pairs):\n",
    "  src[:, i:i+1], trg[:, i:i+1] = tensorsFromPair(pairs[i])\n",
    "\n",
    "batch = (torch.split(src, BATCH_SIZE, dim = 1), torch.split(trg, BATCH_SIZE, dim = 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JFFXvPBZHL11"
   },
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "hid_dim = 512\n",
    "src_n_tokens = input_lang.n_words\n",
    "trg_n_tokens = output_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "5cu5v3epDvyF"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(seq_length = MAX_LENGTH, \n",
    "                  emb_dim = emb_dim,\n",
    "                  hid_dim = hid_dim,\n",
    "                  n_tokens = src_n_tokens,\n",
    "                  num_layers = 1,\n",
    "                  drop_p = 0.5)\n",
    "decoder = Decoder(MAX_LENGTH,\n",
    "                  emb_dim,\n",
    "                  hid_dim,\n",
    "                  trg_n_tokens,\n",
    "                  drop_p = 0)\n",
    "model = Seq2Seq(encoder, decoder, device)\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  logits = model(batch[0][0], batch[1][0]) # test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jpi4cWYAhv1I"
   },
   "source": [
    "## How to train & evaluate model in PyTorch\n",
    "1) model에 적합한 dataset(전체 데이터), dataloader(batch sized data set, iterable) 정의 <br>\n",
    "2) trian(model, loss_fn, optimizer, ...) <br>\n",
    "3) eval(model, loss_fn, optimizer, ...) <br>\n",
    "4) Inference(test dataset) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/PyTorch_framework.PNG\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKBhxH2Vh_38"
   },
   "source": [
    "## 1, 4) 생략\n",
    "## 2, 3) train, eval\n",
    "```python\n",
    "# pseudo code\n",
    "def train_one_epoch():\n",
    "  '''\n",
    "  1) model을 train mode로 변환\n",
    "    (batchnorm, dropout과 같이 train, eval일 때 각기 다른 방식으로 작용하는 layer를 위해 pytorch에서 제공하는 attribute)\n",
    "  2) dataloader 반복적으로 model에 feed\n",
    "  3) outputs = model(dataloader)\n",
    "  4) loss 계산 (loss_fn(outputs, labels))\n",
    "  5) optimizer 초기화\n",
    "  6) back propagation(loss.backward())\n",
    "  7) optimizer step\n",
    "  8) print(loss, accuracy, ...), model save, etc...\n",
    "  '''\n",
    "\n",
    "def evaluate():\n",
    "  '''\n",
    "  1) model을 eval mode로 변환\n",
    "  2) with torch.no_grad():\n",
    "      similar train_one_eopch (except for optimizer)\n",
    "  3) print, model save, etc...\n",
    "  '''\n",
    "```\n",
    "위의 train, eval 함수를 num_epochs 만큼 반복해야하므로 반복문으로 이를 감싸주고 PyTorch 문법에 맞게 변환해주면 됩니다.\n",
    "```python\n",
    "# pseudo code\n",
    "def run(num_epochs, ...):\n",
    "  for epoch in range(num_epochs):\n",
    "    train_one_epoch()\n",
    "    scheulder.step() if scheduler\n",
    "    evaluate()\n",
    "\n",
    "def train_one_epoch(model, train_dl, optimizer, loss_fn, ...):\n",
    "  model.train()\n",
    "  for (inputs, labels) in train_dl:\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  '''\n",
    "    log(loss, accuracy, ...)\n",
    "    model save or not\n",
    "    etc...\n",
    "  '''\n",
    "def evaluate(model, eval_dl, loss_fn, ...):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for (inputs, labels) in eval_dl:\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, -1)\n",
    "      loss = loss_fn(output, labels)\n",
    "\n",
    "      '''\n",
    "        log\n",
    "        model save or not\n",
    "        etc...\n",
    "      '''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMHdeMgBrVnu"
   },
   "source": [
    "## Let's train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "t4xnDNTuxX71"
   },
   "outputs": [],
   "source": [
    "def train(model, batch, optimizer, loss_fn, device, clip = 1 ):\n",
    "  '''\n",
    "  inputs:\n",
    "    - model\n",
    "    - batch(Tuple(src Tensor, trg Tensor)): Tensor.size() = (seq_length, batch_size)\n",
    "    - optimizer\n",
    "    - loss_fn\n",
    "    - device: GPU or CPU\n",
    "    - clip(float): protect gardient exploding by limiting max norm of gradient\n",
    "  outputs:\n",
    "    -  running_loss(float): total loss in train one epoch\n",
    "  '''\n",
    "  model.train()\n",
    "  model = model.to(device)\n",
    "  loss_fn = loss_fn.to(device)\n",
    "  running_loss = 0.0\n",
    "  for i, (src, trg) in enumerate(zip(batch[0], batch[1])):\n",
    "    src, trg = src.to(device), trg.to(device) \n",
    "    logits = model(src, trg) # logits: [seq_length, batch_size,  n_tokens]\n",
    "    n_tokens = logits.size(-1)\n",
    "    logits = logits[1:].reshape(-1, n_tokens) # remove <sos> token and flatten (seq_length * batch_size - 1, n_tokens)\n",
    "    trg = trg[1:].reshape(-1) # (seq_length * batch_size - 1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(logits, trg)\n",
    "    running_loss += loss.item()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    \n",
    "  return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uMC0jeDozsgu"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, batch, loss_fn, device):\n",
    "  '''\n",
    "  inputs:\n",
    "    - model\n",
    "    - batch(Tuple): (src Tensor, trg Tensor) (*Tensor.size() = (seq_length, batch_size))\n",
    "    - loss_fn\n",
    "    - device: GPU or CPU\n",
    "  outputs:\n",
    "    -  running_loss(float): total loss in evaluation one epoch\n",
    "  '''\n",
    "  model.eval()\n",
    "  model = model.to(device)\n",
    "  loss_fn = loss_fn.to(device)\n",
    "  running_loss = 0.0\n",
    "  with torch.no_grad():\n",
    "    for i, (src, trg) in enumerate(zip(batch[0], batch[1])):\n",
    "      src, trg = src.to(device), trg.to(device)\n",
    "      logits = model(src, trg) # logits: [seq_length, batch_size,  n_tokens]\n",
    "      n_tokens = logits.size(-1)\n",
    "      logits = logits[1:].reshape(-1, n_tokens) # remove <sos> token and flatten (seq_length * batch_size - 1, n_tokens)\n",
    "      trg = trg[1:].reshape(-1) # (seq_length * batch_size - 1)\n",
    "      loss = loss_fn(logits, trg)\n",
    "      running_loss += loss.item()\n",
    "\n",
    "  return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6cHKo0os08El"
   },
   "outputs": [],
   "source": [
    "def run(model, train_batch, val_batch, loss_fn, optimizer, num_epochs, device, print_every = 100, clip = 1):\n",
    "  '''\n",
    "  wrapper of train and evaluation\n",
    "  '''\n",
    "  min_loss = float('inf')\n",
    "  best_model = None\n",
    "  for epoch in range(num_epochs):\n",
    "    total_train_loss = train(model, train_batch, optimizer, loss_fn, device, clip)\n",
    "    total_val_loss = evaluation(model, val_batch, loss_fn, device)\n",
    "    if (epoch+1) % print_every == 0 or epoch == 0:\n",
    "      print(f'Epoch| {epoch+1}/{num_epochs}')\n",
    "      print(f'train loss: {total_train_loss/len(train_batch[0])}')\n",
    "      print(f'val loss: {total_val_loss/len(val_batch[0])}')\n",
    "\n",
    "    if min_loss > total_val_loss:\n",
    "      min_loss = total_val_loss\n",
    "      best_model = copy.deepcopy(model)\n",
    "\n",
    "  return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt75Y34sjVVl"
   },
   "source": [
    "## split dataset\n",
    "model을 train할 때 전체 dataset을 train set, validation set으로 나누어야 합니다. <br>\n",
    "train set에서는 실질적인 학습이 일어나고, <br>\n",
    "validation set에서는 model의 성능 평가가 비교적 객관적으로 이루어집니다. <br>\n",
    "만약 train set과 validation set에서 성능 차이가 크게나면 일반적으로 model이 overfitting 되었다고 생각하시면 됩니다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yfejHk3PQKhV"
   },
   "outputs": [],
   "source": [
    "# split data into train, val\n",
    "train_ratio = 0.8\n",
    "train_batch = (batch[0][:int(len(batch[0])*train_ratio)], batch[1][:int(len(batch[0])*train_ratio)])\n",
    "val_batch = (batch[0][int(len(batch[0])*train_ratio):], batch[1][int(len(batch[0])*train_ratio):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "bNz5CdWtRMT4"
   },
   "outputs": [],
   "source": [
    "#loss_fn = nn.CrossEntropyLoss(ignore_index = 2) # ignore pad\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate =0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pRYNUVEQ_ze",
    "outputId": "cdf6f954-8504-4886-800a-015d4fac48e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch| 1/20\n",
      "train loss: 2.0129036840639616\n",
      "val loss: 2.505979177852472\n",
      "Epoch| 2/20\n",
      "train loss: 1.472409933491757\n",
      "val loss: 2.6247515281041465\n",
      "Epoch| 3/20\n",
      "train loss: 1.2924475575748242\n",
      "val loss: 2.564739376306534\n",
      "Epoch| 4/20\n",
      "train loss: 1.239480709088476\n",
      "val loss: 2.505082373817762\n",
      "Epoch| 5/20\n",
      "train loss: 1.1373486481214825\n",
      "val loss: 2.4709080681204796\n",
      "Epoch| 6/20\n",
      "train loss: 1.1013947875876176\n",
      "val loss: 2.413986990849177\n",
      "Epoch| 7/20\n",
      "train loss: 1.0600982069969178\n",
      "val loss: 2.4160922343532243\n",
      "Epoch| 8/20\n",
      "train loss: 1.0114873355940768\n",
      "val loss: 2.4293998181819916\n",
      "Epoch| 9/20\n",
      "train loss: 0.9981789504226886\n",
      "val loss: 2.464413784444332\n",
      "Epoch| 10/20\n",
      "train loss: 0.9680503820118151\n",
      "val loss: 2.4953632528583207\n",
      "Epoch| 11/20\n",
      "train loss: 0.9409303674572392\n",
      "val loss: 2.419907346367836\n",
      "Epoch| 12/20\n",
      "train loss: 0.9479612425753945\n",
      "val loss: 2.4010447710752487\n",
      "Epoch| 13/20\n",
      "train loss: 0.9248014964555439\n",
      "val loss: 2.5098216235637665\n",
      "Epoch| 14/20\n",
      "train loss: 0.9069469542879808\n",
      "val loss: 2.384943505128225\n",
      "Epoch| 15/20\n",
      "train loss: 0.8886521326868158\n",
      "val loss: 2.4789953331152597\n",
      "Epoch| 16/20\n",
      "train loss: 0.8597453164426904\n",
      "val loss: 2.4796039909124374\n",
      "Epoch| 17/20\n",
      "train loss: 0.8495413730019017\n",
      "val loss: 2.4304580440123877\n",
      "Epoch| 18/20\n",
      "train loss: 0.8567408367207175\n",
      "val loss: 2.366885965069135\n",
      "Epoch| 19/20\n",
      "train loss: 0.8345932260939949\n",
      "val loss: 2.309285076955954\n",
      "Epoch| 20/20\n",
      "train loss: 0.7917937056014412\n",
      "val loss: 2.4754027103384337\n"
     ]
    }
   ],
   "source": [
    "best_model = run(model, \n",
    "                 train_batch, \n",
    "                 val_batch, \n",
    "                 loss_fn, \n",
    "                 optimizer, \n",
    "                 num_epochs = 20,\n",
    "                 device = device, \n",
    "                 print_every = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prQsmEzlkBUB"
   },
   "source": [
    "결과값을 보시면 train loss와 val loss의 차이가 크게 나는 것을 아실 수 있습니다. <br>\n",
    "즉, overfitting이 되었는데 이에 대한 추측으로는 <br>\n",
    "> 1) dataset 크기 부족 <br>\n",
    "> 2) dataset이 적절하게 섞이지 않음 <br>\n",
    "> 3) 모델 설계 오류 <br>\n",
    "> 4) teacher forcing에 따른 성능 차이\n",
    "\n",
    "2)의 경우 txt 파일을 보시면 비슷한 문장이 계속 나열되는 것을 아실 수 있습니다. <br>\n",
    "이를 해결하기 위해 먼저 pair를 random shuffle하였기 때문에 어느정도 방지했다고 생각합니다. <br>\n",
    "\n",
    "3)의 경우를 해결하기 위해 dropout, LSTM, optimizer clip 등을 사용하여 overfitting을 방지하였습니다. <br>\n",
    "다른 방안으로는 weight decay, RNN layer 조절 등이 있습니다. <br>\n",
    "ref: https://discuss.pytorch.org/t/simple-encoder-decoder-model-is-overfitting/74632<br>\n",
    "\n",
    "dataset의 크기가 부족하다고는 생각하지 않으므로 teacher focing에 따른 성능 차이로 생각합니다. <br>\n",
    "혹시 문제점을 발견하셔서 알려주시면 감사하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO1W7wjljN1e"
   },
   "source": [
    "## 모델 결과값(validation dataset) 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mddJHtjTxMha",
    "outputId": "bf6a51c1-e260-4089-a466-79777a1183ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence:  il travaille de nuit ce soir . <EOS> <PAD>\n",
      "answer translation:  he is on night duty tonight . <EOS> <PAD>\n",
      "pred translation :  he is afraid of his own . <EOS> <PAD>\n",
      " \n",
      "source sentence:  je ne vais pas travailler . <EOS> <PAD> <PAD>\n",
      "answer translation:  i m not going to work . <EOS> <PAD>\n",
      "pred translation :  i m not going . <EOS> <PAD> <PAD> <PAD>\n",
      " \n",
      "source sentence:  je me specialise en sociologie . <EOS> <PAD> <PAD>\n",
      "answer translation:  i m majoring in sociology . <EOS> <PAD> <PAD>\n",
      "pred translation :  i m going to get married . <EOS> <PAD>\n",
      " \n",
      "source sentence:  tu es l elue . <EOS> <PAD> <PAD> <PAD>\n",
      "answer translation:  you are the one . <EOS> <PAD> <PAD> <PAD>\n",
      "pred translation :  you re the teacher . <EOS> <PAD> <PAD> <PAD>\n",
      " \n",
      "source sentence:  il est deprime . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "answer translation:  he s depressed . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "pred translation :  he is powerful . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      " \n",
      "source sentence:  je ne suis pas tres patiente . <EOS> <PAD>\n",
      "answer translation:  i m not very patient . <EOS> <PAD> <PAD>\n",
      "pred translation :  i m not a saint . <EOS> <PAD> <PAD>\n",
      " \n",
      "source sentence:  nous en avons fini ici . <EOS> <PAD> <PAD>\n",
      "answer translation:  we re finished here . <EOS> <PAD> <PAD> <PAD>\n",
      "pred translation :  we re all retired . <EOS> <PAD> <PAD> <PAD>\n",
      " \n",
      "source sentence:  vous n etes pas invitee . <EOS> <PAD> <PAD>\n",
      "answer translation:  you aren t invited . <EOS> <PAD> <PAD> <PAD>\n",
      "pred translation :  you re not dead yet . <EOS> <PAD> <PAD>\n",
      " \n",
      "source sentence:  tu es fort contrariee . <EOS> <PAD> <PAD> <PAD>\n",
      "answer translation:  you re very upset . <EOS> <PAD> <PAD> <PAD>\n",
      "pred translation :  you re very upset . <EOS> <PAD> <PAD> <PAD>\n",
      " \n",
      "source sentence:  elles sont de retour . <EOS> <PAD> <PAD> <PAD>\n",
      "answer translation:  they re back . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "pred translation :  they re out of town . <EOS> <PAD> <PAD>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_batch = 10\n",
    "pick = random.randint(0, len(val_batch[0]) - 1)\n",
    "with torch.no_grad():\n",
    "  best_model.eval()\n",
    "  best_model = best_model.to(device)\n",
    "  src, trg = val_batch[0][pick], val_batch[1][pick]\n",
    "  src, trg = src.to(device), trg.to(device)\n",
    "  logits = model(src, trg)\n",
    "  preds = logits.argmax(-1)\n",
    "\n",
    "  # [seq_length, batch_size] -> [batch_size, seq_length] (for simply treating sentences)\n",
    "  preds = preds.T\n",
    "  src = src.T\n",
    "  trg = trg.T\n",
    "  \n",
    "  for i in range(test_batch):\n",
    "    input_sentence = [input_lang.index2word[src.item()] for src in src[i]]\n",
    "    answer = [output_lang.index2word[trg.item()] for trg in trg[i]]\n",
    "    pred = [output_lang.index2word[pred.item()] for pred in preds[i]]\n",
    "    print('source sentence: ', ' '.join(input_sentence[1:]))\n",
    "    print('answer translation: ', ' '.join(answer[1:]))\n",
    "    print('pred translation : ', ' '.join(pred[1:]))\n",
    "    print(' ')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Seq2Seq.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
