{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my github: https://github.com/withAnewWorld/models_from_scratch\n",
    "# my blog\n",
    "# https://self-deeplearning.blogspot.com/\n",
    "# https://self-deeplearning.tistory.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqOHdkfQrcGI",
    "outputId": "e583e7b2-13f7-47f8-b824-dd3110d3942c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/translation\n"
     ]
    }
   ],
   "source": [
    "# colab과 google drive를 연동하기 위한 code cell입니다.\n",
    "# google drive를 연동하는 이유는 drive에 저장된 dataset을 가져오기 위함입니다.\n",
    "\n",
    "from google.colab import drive\n",
    "import sys\n",
    "from IPython import display\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "FOLDERNAME = 'translation'\n",
    "sys.path.append('content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "%cd /content/drive/My Drive/$FOLDERNAME\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKIHiiU0mIjI"
   },
   "source": [
    "## Loading data files\n",
    "The data for this project is a set of many thousands of English to French translation pairs.\n",
    "\n",
    "This question on Open Data Stack Exchange <https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>__ pointed me to the open translation site https://tatoeba.org/ which has downloads available at https://tatoeba.org/eng/downloads - and better yet, someone did the extra work of splitting language pairs into individual text files here: https://www.manythings.org/anki/\n",
    "\n",
    "The English to French pairs are too big to include in the repo, so download to data/eng-fra.txt before continuing. The file is a tab separated list of translation pairs:\n",
    "\n",
    "::\n",
    "\n",
    "I am cold.    J'ai froid.\n",
    ".. Note:: **Download the data from here** <https://download.pytorch.org/tutorial/data.zip>_ and extract it to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O_UjhUqm3i-R"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## my module\n",
    "import datasets.language as language\n",
    "import models.seq2seq as seq2seq\n",
    "import utils.engine as engine\n",
    "import utils.visualize_result as visualize_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_Ve0dER1zMp",
    "outputId": "34d74d32-fcc5-4a5f-a56e-541d780ba456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 3383\n",
      "eng 2134\n",
      "['<SOS> il a la cinquantaine . <EOS> <PAD> <PAD> <PAD>', '<SOS> he s in his fifties . <EOS> <PAD> <PAD>']\n"
     ]
    }
   ],
   "source": [
    "## data preprocessing ##\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "input_lang, output_lang, pairs = language.prepareData('eng', \n",
    "                                                      'fra', \n",
    "                                                      MAX_LENGTH, \n",
    "                                                      eng_prefixes, \n",
    "                                                      True, \n",
    "                                                      True)\n",
    "print(random.choice(pairs))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "n_pairs = len(pairs)\n",
    "\n",
    "src = torch.zeros((MAX_LENGTH, n_pairs), dtype = torch.long) \n",
    "trg = torch.zeros((MAX_LENGTH, n_pairs), dtype = torch.long)\n",
    "for i in range(n_pairs):\n",
    "  src[:, i:i+1], trg[:, i:i+1] = language.tensorsFromPair(input_lang, \n",
    "                                                          output_lang, \n",
    "                                                          pairs[i],\n",
    "                                                          device)\n",
    "\n",
    "batch = (torch.split(src, BATCH_SIZE, dim = 1), torch.split(trg, BATCH_SIZE, dim = 1))\n",
    "\n",
    "## model configuration ##\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate =0.001\n",
    "emb_dim = 256\n",
    "hid_dim = 512\n",
    "src_n_tokens = input_lang.n_words\n",
    "trg_n_tokens = output_lang.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtOh_nJ09Vfp"
   },
   "source": [
    "Ref <br>\n",
    "1. embedding <br>\n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html<br>\n",
    "paper: https://arxiv.org/abs/1301.3781\n",
    "2. Seq2Seq <br>\n",
    "0) paper: https://arxiv.org/abs/1409.3215<br>\n",
    "1) Pytorch Seq2Seq Tutorial for Machine Translation(Youtuber: \n",
    "Aladdin Persson): <br>\n",
    "https://www.youtube.com/watch?v=EoGUlvhRYpk&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=38<br>\n",
    "2) github: <br>\n",
    "https://github.com/bentrevett/pytorch-seq2seq <br>\n",
    "3) attention seq2seq github: <br>\n",
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb<br>\n",
    "4) PyTorch tutorial<br>\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html<br>\n",
    "5) seq2seq attention paper <br>\n",
    "https://arxiv.org/abs/1409.0473?context=cs.NE <br>\n",
    "6) TensorFlow seq2seq attention tutorial<br>\n",
    "https://github.com/tensorflow/nmt <br>\n",
    "<br> \n",
    "7) avoid overfitting method(discuss torch): <br>\n",
    "https://discuss.pytorch.org/t/simple-encoder-decoder-model-is-overfitting/74632<br>\n",
    "3. RNN, LSTM <br>\n",
    "1)https://colah.github.io/posts/2015-08-Understanding-LSTMs/ <br>\n",
    "2)https://cs231n.github.io/rnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "1. Attention Seq2Seq 탄생배경\n",
    "2. Attention Seq2Seq architecture\n",
    "3. BiRNN Encoder\n",
    "4. Attention mechanism\n",
    "5. train & eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDvM5oSvZltp"
   },
   "source": [
    "## Attention Seq2Seq\n",
    "* 탄생배경<br>\n",
    "1) 기존 Seq2Seq모델의 경우 source sentence의 정보를 한정된 사이즈의 벡터(context vector)에 압축하기 때문에 문장의 길이가 길어질 경우 정보손실이 너무 많아짐. 이에 따라 모델의 성능 급락<br>\n",
    "2) 모든 문장정보를 압축하기 때문에 각 timestamp에 적합한 단어를 예측하는 데에 어려움을 느낌\n",
    "\n",
    "다음과 같은 문장을 영한 번역하는 과정을 가정해보겠습니다. <br>\n",
    "source: I went to hospital. Because I had to get the medicine <br>\n",
    "target: 나는 병원에 갔었다. 약을 처방받아야 했었기 때문에. <br>\n",
    "\n",
    "Decoder RNN에 따라 다음의 단어를 예측해야 하는 상황을 가정해봅시다. <br>\n",
    "나는 병원에 **(   &nbsp;&nbsp; &nbsp; )** <br>\n",
    "이 때, 기존의 Seq2Seq의 경우 source sentence의 맥락과 이전 hidden state의 정보만을 가지고 단어를 예측해야 합니다. <br>\n",
    "하지만 source sentence의 각 단어에 대해 mapping되는 확률을 구하여 번역을 진행하면 더 성능이 높아지지 않을까요? <br>\n",
    "ex) **(&nbsp; &nbsp; &nbsp; )**에 해당하는 source sentence의 단어 확률 <br>\n",
    "\n",
    "I: 5% <br>\n",
    "went: 60% <br>\n",
    "to: 20% <br>\n",
    "hospital: 10% <br>\n",
    "... <br>\n",
    "위의 내용을 알고리즘으로 구현한 것이 Attention mechanism입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwD4bI0oAdpT"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/attn_expression.PNG\">\n",
    "</p>\n",
    "\n",
    "<div align = 'center'> 자료출처: https://github.com/tensorflow/nmt </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/attn_seq2seq.PNG\">\n",
    "</p>\n",
    "\n",
    "<div align = 'center'> 자료출처: https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqLhAObi0XmP"
   },
   "source": [
    "## Attention Seq2Seq process\n",
    "$Output = DecoderRNN(hidden\\ state,\\ concat(embedded,\\ context\\ vector))$ <br>\n",
    "$logits = Linear(concat(Embedded,\\ Output,\\ Context\\ Vector))$ <br>\n",
    "\n",
    "cf) 논문(Bahdanau)의 구현을 따라서 Encoder의 경우 BiRNN(Bidirectional RNN)을 사용했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vb1qXZG61rpT"
   },
   "source": [
    "## BiRNN Encoder\n",
    "기존의 RNN은 한방향으로 정보가 흐르기 때문에 다음과 같은 문제가 발생합니다. <br>\n",
    "<br>\n",
    "I want _ <br>\n",
    "\n",
    "여러분도 아시듯 빈칸에 들어갈 수 있는 말은 굉장히 많습니다. <br>\n",
    "왜냐하면 문장의 전체 맥락이 주어지지 않아 단어를 한정할 수 없기 때문입니다. <br>\n",
    "하지만 뒤의 단어에 대한 정보가 다음과 같이 추가로 주어질 경우 빈칸에 들어갈 단어를 한정할 수 있습니다. <br>\n",
    "I want _ go home. &nbsp; (answer: to)<br>\n",
    "\n",
    "BiRNN은 양방향으로 정보를 받아들이기 때문에 위의 예시와 같이 모델링 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/BiRNN.PNG\">\n",
    "</p>\n",
    "\n",
    "<div align = 'center'> 자료출처: https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/BiRNN_expression.PNG\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4wYM97q4Qa0"
   },
   "source": [
    "$encoder\\ outs,\\ h_n =\\ BiRNN\\ GRU(input)$<br>\n",
    "<br>\n",
    "* size <br>\n",
    "$input(seq\\ length,\\ batch\\ size,\\ hidden\\ dim)$ <br>\n",
    "<br>\n",
    "$encoder\\ outs(seq\\ length,\\ batch\\ size,\\ 2*hidden\\ dim)$ <br>\n",
    "$h_n(2*num\\ layers,\\ batch\\ size,\\ hidden\\ dim)$ <br>\n",
    "\n",
    "Decoder의 경우 Encoder와 달리 기본적인 RNN(GRU)이기 때문에 size를 다음과 같이 맞춰줘야 합니다. <br>\n",
    "1) input(batch size, hidden_dim) <br>\n",
    "2) hidden(batch_size, hidden_dim) <br>\n",
    "3) encoder outs(seq_length, batch_size, 2 x hidden_dim) <br>\n",
    "\n",
    "size가 다른 경우는 hidden state만 있으므로 h_n을 concatenation한 후 마지막 차원(2 x hidden_dim)을 바꾸기 위해 linear layer를 사용합니다.<br>\n",
    "```python\n",
    "fc = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "\n",
    "out = fc(concatenation((h_n[0], h_n[1]), dim = -1))\n",
    "# 학습의 용이성을 위해 activation function 삽입\n",
    "hidden = activation(out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MDb40Wt9E05U"
   },
   "outputs": [],
   "source": [
    "class BiRNNEncoder(nn.Module):\n",
    "  def __init__(self, \n",
    "               n_tokens,\n",
    "               emb_dim, \n",
    "               hid_dim,\n",
    "               n_layers = 1,\n",
    "               drop_p = 0.3):\n",
    "    super(BiRNNEncoder, self).__init__()\n",
    "    self.embedding = nn.Embedding(n_tokens, emb_dim)\n",
    "    self.dropout = nn.Dropout(drop_p)\n",
    "    self.rnn = nn.GRU(emb_dim, \n",
    "                      hid_dim, \n",
    "                      n_layers,\n",
    "                      bidirectional = True)\n",
    "\n",
    "    self.decoder_fc = nn.Linear(2 * hid_dim, hid_dim)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    embedded = self.dropout(self.embedding(x))\n",
    "    out, hidden = self.rnn(embedded)\n",
    "    hidden = torch.cat((hidden[0], hidden[1]), dim = -1) # (batch_size, hid_dim * 2)\n",
    "    hidden = torch.tanh(self.decoder_fc(hidden))\n",
    "\n",
    "    return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3308zsnGJ29"
   },
   "source": [
    "## Attention mechanism\n",
    "Encoder의 경우 맨 마지막 hidden state(정방향, 역방향)만 반환하므로 각 timestamp에 대한 정보는 encoder의 output을 통해서 구할 수 있습니다. <br>\n",
    "즉 $h_{s}$는 encoder outputs, $h_{t}$는 decoder의 이전 hidden state를 통해 attention score를 구할 수 있습니다.<br>\n",
    "\n",
    "cf) Bahdanau Attention mechanism 사용 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/attn_expression.PNG\">\n",
    "</p>\n",
    "\n",
    "<div align = 'center'> 자료출처: https://github.com/tensorflow/nmt</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM2oNxxkF0Tq"
   },
   "source": [
    "```python\n",
    "#pseudo code\n",
    "def attention_score(encoder outs, decoder prev hidden):\n",
    "  '''\n",
    "    decoder prev hidden(batch_size, hidden_dim)\n",
    "    encoder outs(seq_length, batch_size, 2*hidden_dim)\n",
    "  '''\n",
    "  \n",
    "  return v(tanh(w1(decoder prev hidden) + w2(encoder outs))) \n",
    "\n",
    "  # encoder outs과 decoder prev hidden의 size를 맞춰줘야 하므로 w1, w2의 out features를 동일하게 설정 \n",
    "  # v.out_features = 1\n",
    "\n",
    "def attention weight(attention_score):\n",
    "  '''\n",
    "    attention_score(seq_length, batch_size, 1)\n",
    "  '''\n",
    "\n",
    "  return F.softmax(attention_score, dim = 0) # (각 sequence data에 대한 확률을 구해야 하므로 dim = 0)\n",
    "\n",
    "def context_vector(attention_weight, encoder_outs):\n",
    "  '''\n",
    "    attention_weight(seq_length, batch_size, 1)\n",
    "    encoder_outs(seq_length, batch_size, 2 * hidden_dim)\n",
    "  '''\n",
    "\n",
    "  return sum(attention_weight * encoder_outs, dim = 0) # attention_weight(..., 1) -> (..., 2 * hidden_dim) (by broadcasting semantics)\n",
    "  # weighted sum(가중합)을 sequential data에 따라 연산하므로 dim = 0\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nd-YMfMZJHVP"
   },
   "source": [
    "Attention mechanism을 통해 구한 context vector는 <br>\n",
    "1) Decoder RNN의 embedded와 함께 input으로 <br>\n",
    "2) Decoder RNN결과로 산출된 output vector와 embedded와 통합되어 linear layer를 거친 후 logits을 산출하게 됩니다. <br>\n",
    "\n",
    "```python\n",
    "# pseudo code\n",
    "Encoder_input = concatenation(embedded, context vector)\n",
    "out, hidden = EncoderRNN(Encoder_input, hidden)\n",
    "out = concatenation(out, embedded, context vector)\n",
    "logits = Linear(out)\n",
    "\n",
    "```\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "l8Q2-FnCFIGL"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, \n",
    "               hid_dim):\n",
    "    super(Attention, self).__init__()\n",
    "    self.W1 = nn.Linear(hid_dim, hid_dim)\n",
    "    self.W2 = nn.Linear(2 * hid_dim, hid_dim)\n",
    "    self.v = nn.Linear(hid_dim, 1)\n",
    "\n",
    "  def forward(self, dec_hidden, enc_outs):\n",
    "    '''\n",
    "    inputs:\n",
    "      - dec_hidden(Tensor[batch_size, hid_dim]) \n",
    "      - enc_outs(Tensor[seq_length, batch_size, 2 * hid_dim])\n",
    "    outputs:\n",
    "      - attn_weight(Tensor[seq_length, batch_size, 1])\n",
    "      - context_vector(Tensor[batch_size, 2 * hid_dim])\n",
    "    '''\n",
    "    score = self.v(torch.tanh(self.W1(dec_hidden) + self.W2(enc_outs))) # (seq_length, bath_size, 1)\n",
    "    attn_weight = F.softmax(score, dim = 0) # (seq_length, batch_size, 1)\n",
    "    context_vector = torch.sum((attn_weight * enc_outs), dim = 0) # (batch_size, 2 * hid_dim)\n",
    "    return attn_weight, context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vNJrtwI2FOp_"
   },
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               emb_dim,\n",
    "               hid_dim,\n",
    "               n_tokens,\n",
    "               n_layers = 1,\n",
    "               drop_p = 0):\n",
    "    super(AttnDecoder, self).__init__()\n",
    "\n",
    "    self.rnn = nn.GRU(emb_dim + 2*hid_dim, hid_dim)\n",
    "    self.embedding = nn.Embedding(n_tokens, emb_dim)\n",
    "    self.dropout = nn.Dropout(drop_p)\n",
    "    self.attn = Attention(hid_dim)\n",
    "    self.fc = nn.Linear(3*hid_dim + emb_dim, n_tokens)\n",
    "    self.trg_n_tokens = n_tokens\n",
    "\n",
    "  def forward(self, input, hidden, enc_outs):\n",
    "    '''\n",
    "    inputs:\n",
    "      - input(Tensor[batch_size]):\n",
    "      - hidden(Tensor[batch_size, hid_dim])\n",
    "      - enc_outs(Tensor[seq_length, batch_size, hid_dim * 2])\n",
    "    outputs:\n",
    "      - logits(Tensor[batch_size, trg_n_tokens])\n",
    "      - hidden(Tensor[batch_size, hid_dim])\n",
    "    '''\n",
    "    \n",
    "    input = input.unsqueeze(0)\n",
    "    embedded = self.dropout(self.embedding(input)) # (1, batch_size, emb_dim)\n",
    "    attn_weight, context_vector = self.attn(hidden, enc_outs)\n",
    "    x = torch.cat((context_vector.unsqueeze(0), embedded), dim = 2) # (1, batch_size, emb_dim + 2 * hid_dim)\n",
    "    out, hidden = self.rnn(x, hidden.unsqueeze(0)) # out, hidden: (1, batch_size, hid_dim)  \n",
    "    \n",
    "    # out: (1, bs, hid_dim), context: (bs, 2*hid_dim), embedded: (1, bs, emb_dim)\n",
    "    out = torch.cat((out.squeeze(0), context_vector, embedded.squeeze(0)), dim = -1)\n",
    "    logits = self.fc(out)\n",
    "    return logits.squeeze(0), hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5cu5v3epDvyF"
   },
   "outputs": [],
   "source": [
    "encoder = BiRNNEncoder(n_tokens = src_n_tokens,\n",
    "                  emb_dim = emb_dim,\n",
    "                  hid_dim = hid_dim,\n",
    "                  n_layers = 1,\n",
    "                  drop_p = 0.5)\n",
    "decoder = AttnDecoder(emb_dim,\n",
    "                      hid_dim,\n",
    "                      trg_n_tokens,\n",
    "                      drop_p = 0)\n",
    "model = seq2seq.Seq2Seq(encoder, decoder, device)\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  logits = model(batch[0][0], batch[1][0]) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yfejHk3PQKhV"
   },
   "outputs": [],
   "source": [
    "# split data into train, val\n",
    "train_ratio = 0.8\n",
    "train_batch = (batch[0][:int(len(batch[0])*train_ratio)], batch[1][:int(len(batch[0])*train_ratio)])\n",
    "val_batch = (batch[0][int(len(batch[0])*train_ratio):], batch[1][int(len(batch[0])*train_ratio):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pRYNUVEQ_ze",
    "outputId": "d52e9b64-3d7f-4ab4-f3ed-6b18a9a768ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch| 1/20\n",
      "train loss: 1.2633036498960695\n",
      "val loss: 4.335380499561627\n",
      "Epoch| 2/20\n",
      "train loss: 0.8984831243753433\n",
      "val loss: 4.003439361850421\n",
      "Epoch| 3/20\n",
      "train loss: 0.8271279609517047\n",
      "val loss: 3.939225440224012\n",
      "Epoch| 4/20\n",
      "train loss: 0.7263752755365873\n",
      "val loss: 3.906046763062477\n",
      "Epoch| 5/20\n",
      "train loss: 0.7293283071957136\n",
      "val loss: 3.8823514928420386\n",
      "Epoch| 6/20\n",
      "train loss: 0.666190715212571\n",
      "val loss: 3.714456950624784\n",
      "Epoch| 7/20\n",
      "train loss: 0.6164490760941255\n",
      "val loss: 3.7301204750935235\n",
      "Epoch| 8/20\n",
      "train loss: 0.5985076035323896\n",
      "val loss: 3.6563456505537033\n",
      "Epoch| 9/20\n",
      "train loss: 0.572482169145032\n",
      "val loss: 3.6340570598840714\n",
      "Epoch| 10/20\n",
      "train loss: 0.5806366271094272\n",
      "val loss: 3.6957877377669015\n",
      "Epoch| 11/20\n",
      "train loss: 0.5597309899173285\n",
      "val loss: 3.5582362562417984\n",
      "Epoch| 12/20\n",
      "train loss: 0.5800884277412766\n",
      "val loss: 3.5537956009308496\n",
      "Epoch| 13/20\n",
      "train loss: 0.5491896463852179\n",
      "val loss: 3.4604779109358788\n",
      "Epoch| 14/20\n",
      "train loss: 0.5052413984348899\n",
      "val loss: 3.55977401137352\n",
      "Epoch| 15/20\n",
      "train loss: 0.5056120734857885\n",
      "val loss: 3.465335508187612\n",
      "Epoch| 16/20\n",
      "train loss: 0.48348020346541154\n",
      "val loss: 3.5597613404194512\n",
      "Epoch| 17/20\n",
      "train loss: 0.48405920243576955\n",
      "val loss: 3.554446587959925\n",
      "Epoch| 18/20\n",
      "train loss: 0.4618798965686246\n",
      "val loss: 3.5284831126530967\n",
      "Epoch| 19/20\n",
      "train loss: 0.4502935789917645\n",
      "val loss: 3.641504690051079\n",
      "Epoch| 20/20\n",
      "train loss: 0.44912007937305853\n",
      "val loss: 3.6138735761245093\n"
     ]
    }
   ],
   "source": [
    "best_model = engine.run(model, \n",
    "                 train_batch, \n",
    "                 val_batch, \n",
    "                 loss_fn = nn.CrossEntropyLoss(), \n",
    "                 optimizer = optim.Adam(model.parameters(), lr = learning_rate), \n",
    "                 num_epochs = 20,\n",
    "                 device = device, \n",
    "                 print_every = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO1W7wjljN1e"
   },
   "source": [
    "## 모델 결과값(validation dataset) 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQ7XCrF77n4-",
    "outputId": "d5194452-5115-49bd-98b1-c54cff989059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence:  j ai des prejuges . <EOS> <PAD> <PAD> <PAD>\n",
      "answer translation:  i m prejudiced . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "pred translation :  i m very shy . <EOS> <PAD> <PAD> <PAD>\n",
      " \n",
      "source sentence:  c est un bon nageur . <EOS> <PAD> <PAD>\n",
      "answer translation:  he is a good swimmer . <EOS> <PAD> <PAD>\n",
      "pred translation :  he is a good good . <EOS> <PAD> <PAD>\n",
      " \n",
      "source sentence:  il est mon ami . <EOS> <PAD> <PAD> <PAD>\n",
      "answer translation:  he is my friend . <EOS> <PAD> <PAD> <PAD>\n",
      "pred translation :  he s my friend . <EOS> <PAD> <PAD> <PAD>\n",
      " \n",
      "source sentence:  vous etes une opportuniste . <EOS> <PAD> <PAD> <PAD>\n",
      "answer translation:  you re opportunistic . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "pred translation :  you re opportunistic . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      " \n",
      "source sentence:  je suis completement epuise . <EOS> <PAD> <PAD> <PAD>\n",
      "answer translation:  i m completely exhausted . <EOS> <PAD> <PAD> <PAD>\n",
      "pred translation :  i m completely exhausted . <EOS> <PAD> <PAD> <PAD>\n",
      " \n",
      "source sentence:  elles sont speciales . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "answer translation:  they re special . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "pred translation :  they re small . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      " \n",
      "source sentence:  il porte un chapeau . <EOS> <PAD> <PAD> <PAD>\n",
      "answer translation:  he is wearing a hat . <EOS> <PAD> <PAD>\n",
      "pred translation :  he s wearing a hat . <EOS> <PAD> <PAD>\n",
      " \n",
      "source sentence:  j essaie simplement de survivre . <EOS> <PAD> <PAD>\n",
      "answer translation:  i m just trying to survive . <EOS> <PAD>\n",
      "pred translation :  i m just trying coming back . <EOS> <PAD>\n",
      " \n",
      "source sentence:  on dit qu il est riche . <EOS> <PAD>\n",
      "answer translation:  he is said to be rich . <EOS> <PAD>\n",
      "pred translation :  he is said to rich . <EOS> <PAD> <PAD>\n",
      " \n",
      "source sentence:  vous etes tous prets . <EOS> <PAD> <PAD> <PAD>\n",
      "answer translation:  you re all set . <EOS> <PAD> <PAD> <PAD>\n",
      "pred translation :  you re all set . <EOS> <PAD> <PAD> <PAD>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "visualize_result.print_val(best_model,\n",
    "                           input_lang,\n",
    "                           output_lang,\n",
    "                           val_batch,\n",
    "                           device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZKIHiiU0mIjI",
    "QDvM5oSvZltp",
    "mwD4bI0oAdpT",
    "EqLhAObi0XmP",
    "cO1W7wjljN1e"
   ],
   "name": "AttnSeq2Seq.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
